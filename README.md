# GGAR-Gradient-Guided-Adaptive-Regularization--deep-learning---classification-
# CDS Filtering Pipeline for Brassica Species
This repository contains a Python script (CDS_Data_Validation.py) to filter coding sequences (CDS) from four Brassica species (B. juncea, B. napus, B. oleracea, B. rapa) downloaded from EnsemblPlants (release-61). The script validates FASTA-formatted CDS against strict biological criteria:

Length divisible by 3

Standard DNA bases only (A/T/C/G)

Valid start (ATG) and stop codons (TAA, TAG, TGA)

No internal stop codons

Correct translation to amino acids

Data Sources
Raw CDS files were retrieved from:

B. juncea: ftp.ensemblgenomes.ebi.ac.uk/pub/plants/release-61/fasta/brassica_juncea/cds/

B. napus: ftp.ensemblgenomes.ebi.ac.uk/pub/plants/release-61/fasta/brassica_napus/cds/

B. oleracea: ftp.ensemblgenomes.ebi.ac.uk/pub/plants/release-61/fasta/brassica_oleracea/cds/

B. rapa: ftp.ensemblgenomes.ebi.ac.uk/pub/plants/release-61/fasta/brassica_rapa/cds/
# Run the script:
python CDS_Data_Validation.py
Input: FASTA files in input_dir (configured in script).

Output: Filtered FASTA files + statistics CSV (filtering_stats.csv).

Dependencies
Python 3.x

Biopython (pip install biopython)

Output
Filtered sequences: Valid CDS per species (.fasta).

Statistics: Detailed filtering metrics (pass/fail counts per criterion).
Codon Frequency Analysis
# The script CDS_Codon_Freq_CSV.py computes codon usage frequencies from filtered CDS sequences (generated by CDS_Data_Validation.py) and outputs a combined CSV file for downstream analysis.

Workflow
Input: Filtered FASTA files (from Final_Filtered directory).

Processing:

Counts occurrences of all 64 codons per sequence.

Labels species numerically (sorted by sequence count, descending).

Output: A CSV file (combined_codon_frequencies_labeled.csv) with columns:

Species: Original species name (e.g., Brassica_napus.AST_PRJEB5043_v1.cds.all_filtered).

Label: Numeric ID (1 = highest sequence count).

Sequence_ID: FASTA record identifier.

Columns for all 64 codons (e.g., AAA, AAC, ..., TTT) with raw counts.

Example Output (First 3 Rows)
Species	                               Label	Sequence_ID	  AAA	AAC	...	TTT
Brassica_napus.AST_PRJEB5043_v1.cds...	1	     CDY69013	      6	6	...	   4
Brassica_napus.AST_PRJEB5043_v1.cds...	1	     CDY71688	     10	8	...	   6
Brassica_napus.AST_PRJEB5043_v1.cds...	1	     CDY71689	     6	1	...	   3
(Full CSV includes all 64 codons and all sequences.)
# Usage
python CDS_Codon_Freq_CSV.py
Input Directory: Configure input_dir in the script (default: Final_Filtered).

Output: CSV saved to input_dir/combined_codon_frequencies_labeled.csv.

Dependencies
Python 3.x

Biopython (pip install biopython)

Pandas (pip install pandas)

Notes
Reproducibility: The script automatically sorts species by sequence count (descending) and assigns labels.

Integration: Use this CSV as input for statistical analyses or visualizations (e.g., PCA, t-SNE).
# Comparative Analysis of Regularization Methods
The script GGAR_MLP_and_Variants.py evaluates six neural network architectures with different regularization strategies for classifying Brassica species based on codon frequencies.

# Key Methods
GGAR: Gradient-Guided Adaptive Regularizer (novel).

AdaptiveL1L2: Fixed L1 + L2 regularization.

FixedL1/L2/ElasticNet: Standard regularization baselines.

MLP: Deep multilayer perceptron with L2 regularization.

# Workflow
Input: Preprocessed CSV combined_codon_frequencies_labeled.csv named to a new file with new name "4_Species.csv", by removing column names Species and Sequence_ID and assign Label a new name Species, with codon counts (columns) and species labels (1-4).

# Training:

10-fold cross-validation.

Evaluates multiple learning rates (0.01, 0.001, 0.0001) and batch sizes (32, 64, 128, 256).

Early stopping (patience=10) to prevent overfitting.

# Outputs (per configuration):

Metrics: Accuracy, precision, recall, F1, MCC (based on average of 10 folds train/val/test).

Confusion matrices: PNG files.

Best model: Saved as .h5 file.

Training logs: Fold-wise metrics and timing.

# Example Output Structure
results/
├── lr_0_001_bs_64/
│   ├── results_GGAR/
│   │   ├── Best_Model_Metrics.csv
│   │   ├── Confusion_Matrix.png
│   │   └── (other metrics CSVs)
│   └── (other methods...)
└── (other hyperparameter combinations...)
# Usage
1. Configure paths in the script:
   DATA_PATH = 'path/to/4_Species.csv'  # Input CSV
BASE_DIR = 'path/to/results'         # Output directory
2. Run
   python GGAR_MLP_and_Variants.py
  #  Dependencies
Python 3.x

TensorFlow 2.x

Pandas, NumPy, Scikit-learn, Matplotlib, Seaborn
CSV Aggregation Pipeline
# The script combine_csvs.py consolidates results from all model configurations (72 total combinations of learning rates and batch sizes) generated by GGAR_MLP_and_Variants.py into unified CSV files for downstream analysis.

# Key Features
Input: Scans the results/ directory for fold-wise metrics (e.g., Best_Model_Metrics.csv, Average_Metrics.csv) across all hyperparameter combinations.

# Metadata Enrichment: Adds columns for:

learning_rate (e.g., 0.001)

batch_size (e.g., 64)

method (e.g., GGAR, MLP)

 Output: Saves combined CSVs to results/Combined_CSVs/, preserving the original filenames (e.g., Best_Model_Metrics.csv now contains all configurations).

# Usage
1. Ensure GGAR_MLP_and_Variants.py has generated results in the expected directory structure.

2. Run:
python combine_csvs.py
Use the combined CSVs (e.g., for statistical tests or visualization).
# Output Structure
results/  
└── Combined_CSVs/  
    ├── Best_Model_Metrics.csv       # All best-model metrics  
    ├── Average_Metrics.csv          # All fold-averaged metrics  
    └── (other combined files...)  
#  Purpose
Enables batch analysis of all models (e.g., comparing GGAR vs baselines across hyperparameters).

Eliminates manual CSV merging for large-scale experiments.
# Statistical Comparison of Regularization Methods
The script Anova_Kruskal_Wallis.py performs comprehensive statistical comparisons across all trained models (GGAR vs baselines) using:

ANOVA (for normally distributed data with equal variances)

Kruskal-Wallis (non-parametric alternative)

# Key Features
Input: Uses combined metrics from results/Combined_CSVs/Fold_Metrics_Detailed.csv.

# Tests:

Checks normality (Shapiro-Wilk) and homogeneity of variance (Levene’s test).

Identifies the best-performing method per metric (accuracy/precision/recall/F1/MCC) for each hyperparameter combination.

# Output: Saves results to Comparison_ANOVA_Kruskal.csv with columns:

Learning_Rate, Batch_Size, Metric (e.g., Test_Accuracy).

Best_Method, p_value, Significant (p < 0.05).

Mean differences between top methods.
# Useage
python Anova_Kruskal_Wallis.py  
Prerequisites: Ensure combine_csvs.py has generated the input CSV.
# Example Output
Learning_Rate	Batch_Size	Metric	Best_Method	p_value	Significant
0.001	64	Test_Accuracy	GGAR	0.003	True
# Purpose
Quantifies significant performance differences between regularization strategies.
# Training Dynamics Visualization
The script Plots_Accuracy_Loss.py generates comparative plots of training/validation metrics across all regularization methods (GGAR, AdaptiveL1L2, etc.) for a specific hyperparameter combination (LR=0.0001, BS=32), Note: you can draw for any learning rate and batch size by changing the name of the input folder.

# Key Features
Input: Loads fold-wise training histories from results/lr_0_0001_bs_32/.

# Output:

Figure 3 (Manuscript): Training  validation accuracy and loss curves.

Figure 4 (Manuscript): Training validation accuracy and loss curves.

Saves as training_dynamics.png with:

Mean trajectories (solid lines) ± 1 standard deviation (shaded regions).

Consistent color-coding across methods.

# Usage
Ensure GGAR_MLP_and_Variants.py has been executed for the target hyperparameters.

Run:

bash
python Plots_Accuracy_Loss.py  
Note: Configure BASE_DIR in the script if analyzing other LR/BS combinations.

# Visualization Details
Methods:

GGAR (red), AdaptiveL1L2 (green), FixedL1 (blue), FixedL2 (orange), ElasticNet (purple), MLP (cyan).

# Metrics:

Smoothed curves (edge padding for consistent epoch lengths).

Standard deviation bands from 10-fold cross-validation.

# Example Output
https://results/lr_0_0001_bs_32/training_dynamics.png

# Dependencies
Python 3.x

Matplotlib, Seaborn, NumPy

Guides hyperparameter selection by identifying robust configurations.


